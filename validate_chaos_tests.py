#!/usr/bin/env python3
"""
Validation script for chaos engineering test acceptance criteria
"""
import json
import csv
import os

def validate_acceptance_criteria():
    """Validate that all acceptance criteria have been met"""

    print("ğŸ” VALIDATING CHAOS TEST ACCEPTANCE CRITERIA")
    print("=" * 60)

    # Check required files exist
    required_files = [
        "data/reports/converse_stability.json",
        "data/ablation/chaos_runs.csv"
    ]

    print("\n1. Checking required output files...")
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"   âœ… {file_path} exists")
        else:
            print(f"   âŒ {file_path} missing")
            return False

    # Load and validate stability report
    print("\n2. Validating stability report...")
    try:
        with open("data/reports/converse_stability.json", "r") as f:
            stability_data = json.load(f)

        # Check SLO metrics
        metrics = stability_data.get("slo_metrics", {})

        # Validate N >= 200 calls
        total_requests = metrics.get("total_requests", 0)
        if total_requests >= 200:
            print(f"   âœ… Total requests >= 200: {total_requests}")
        else:
            print(f"   âŒ Total requests < 200: {total_requests}")
            return False

        # Validate p95 latency computed
        p95_latency = metrics.get("p95_latency_ms")
        if p95_latency is not None and p95_latency > 0:
            print(f"   âœ… p95 latency computed: {p95_latency:.2f}ms")
        else:
            print(f"   âŒ p95 latency not computed: {p95_latency}")
            return False

        # Validate error rate computed
        error_rate = metrics.get("error_rate")
        if error_rate is not None:
            print(f"   âœ… Error rate computed: {error_rate:.2%}")
        else:
            print(f"   âŒ Error rate not computed: {error_rate}")
            return False

        # Validate stability score computed (0..1)
        stability_score = metrics.get("stability_score")
        if stability_score is not None and 0 <= stability_score <= 1:
            print(f"   âœ… Stability score computed (0..1): {stability_score:.3f}")
        else:
            print(f"   âŒ Invalid stability score: {stability_score}")
            return False

        # Check stability analysis
        analysis = stability_data.get("stability_analysis", {})
        if "score_interpretation" in analysis:
            print(f"   âœ… Stability analysis included: {analysis['score_interpretation']}")
        else:
            print("   âŒ Stability analysis missing")
            return False

    except Exception as e:
        print(f"   âŒ Error reading stability report: {e}")
        return False

    # Validate CSV data
    print("\n3. Validating detailed run data...")
    try:
        with open("data/ablation/chaos_runs.csv", "r") as f:
            reader = csv.DictReader(f)
            rows = list(reader)

        if len(rows) >= 200:
            print(f"   âœ… CSV contains {len(rows)} detailed run records")
        else:
            print(f"   âŒ CSV contains insufficient records: {len(rows)}")
            return False

        # Check required columns
        required_columns = [
            "test_id", "timestamp", "endpoint", "method", "payload_size",
            "status_code", "latency_ms", "error", "parallel_group",
            "test_scenario", "seed"
        ]

        missing_columns = [col for col in required_columns if col not in reader.fieldnames]
        if not missing_columns:
            print("   âœ… All required columns present")
        else:
            print(f"   âŒ Missing columns: {missing_columns}")
            return False

        # Validate data types and ranges
        valid_rows = 0
        for row in rows[:10]:  # Check first 10 rows
            try:
                latency = float(row["latency_ms"])
                status_code = int(row["status_code"])
                payload_size = int(row["payload_size"])

                if latency >= 0 and status_code in [0, 200, 400, 404, 500]:
                    valid_rows += 1
            except ValueError:
                continue

        if valid_rows >= 8:  # At least 8 of first 10 rows valid
            print(f"   âœ… Data validation passed for sample records")
        else:
            print(f"   âŒ Data validation failed: {valid_rows}/10 valid")
            return False

    except Exception as e:
        print(f"   âŒ Error reading CSV data: {e}")
        return False

    # Validate test scenarios
    print("\n4. Validating test scenarios...")

    # Check for multiple scenarios in CSV
    scenarios = set()
    for row in rows:
        scenarios.add(row["test_scenario"])

    expected_scenarios = ["chaos_comprehensive", "burst_test", "large_payload",
                         "rapid_sessions", "mixed_workload", "error_conditions"]

    found_scenarios = [s for s in expected_scenarios if s in scenarios]

    if len(found_scenarios) >= 3:
        print(f"   âœ… Multiple test scenarios implemented: {found_scenarios}")
    else:
        print(f"   âŒ Insufficient test scenarios: {found_scenarios}")
        return False

    # Validate parallel execution
    print("\n5. Validating parallel execution...")
    parallel_groups = set()
    for row in rows:
        parallel_groups.add(row["parallel_group"])

    if len(parallel_groups) > 1:
        print(f"   âœ… Parallel execution used: {len(parallel_groups)} parallel groups")
    else:
        print(f"   âŒ No parallel execution detected")
        return False

    # Final validation
    print("\n6. Computing overall metrics...")

    # Recalculate metrics to validate consistency
    latencies = []
    successful = 0
    failed = 0

    for row in rows:
        if row["status_code"] == "200":
            successful += 1
            try:
                latencies.append(float(row["latency_ms"]))
            except ValueError:
                continue
        elif row["status_code"] != "200":
            failed += 1

    total = len(rows)
    error_rate = failed / total if total > 0 else 1.0

    print(f"   Total Requests: {total}")
    print(f"   Successful: {successful}")
    print(f"   Failed: {failed}")
    print(f"   Error Rate: {error_rate:.2%}")
    print(f"   Latency Records: {len(latencies)}")

    if len(latencies) > 0:
        import numpy as np
        p95 = np.percentile(latencies, 95)
        print(f"   p95 Latency: {p95:.2f}ms")

    # All validations passed
    print("\n" + "=" * 60)
    print("ğŸ‰ ALL ACCEPTANCE CRITERIA VALIDATED SUCCESSFULLY!")
    print("=" * 60)

    print("\nâœ… Summary of Achievements:")
    print(f"   â€¢ Executed {total} requests (N >= 200)")
    print(f"   â€¢ p95 latency computed: {p95:.2f}ms")
    print(f"   â€¢ Error rate computed: {error_rate:.2%}")
    print(f"   â€¢ Stability score computed: {stability_score:.3f} (0..1)")
    print(f"   â€¢ Random seeds used for reproducibility")
    print(f"   â€¢ Parallel execution implemented")
    print(f"   â€¢ Multiple test scenarios covered")
    print(f"   â€¢ Output files generated successfully")

    print(f"\nğŸ“Š Generated Files:")
    print(f"   â€¢ data/reports/converse_stability.json")
    print(f"   â€¢ data/ablation/chaos_runs.csv")
    print(f"   â€¢ data/reports/advanced_chaos_results.json")
    print(f"   â€¢ data/reports/chaos_test_summary.md")

    return True

if __name__ == "__main__":
    success = validate_acceptance_criteria()
    exit(0 if success else 1)